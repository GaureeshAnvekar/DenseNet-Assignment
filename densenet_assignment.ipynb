{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "densenet_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaureeshAnvekar/DenseNet-Assignment/blob/master/densenet_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K70hAckqg0EA",
        "colab_type": "code",
        "outputId": "a1e3a37b-dd17-44f3-d1fa-2a797cd3d2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIx_KIigxPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All the necessary imports\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Flatten, Input, AveragePooling2D, merge, Activation, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive                                    # This is to mount google drive with colab to save checkpoints in google drive.\n",
        "from keras.models import load_model\n",
        "from keras.utils import Sequence\n",
        "import numpy as np\n",
        "import math as m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCbFkKYe2hA2",
        "colab_type": "code",
        "outputId": "142b7312-87e8-4842-9ed4-f4c55799e672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount google drive with colab\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNHw6luQg3gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsO_yGxcg5D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 250\n",
        "num_layers = 12              # No.of layers in each dense-block\n",
        "num_filter = 12\n",
        "compression = 0.5\n",
        "l2_reg = 1e-4                #lambda for L2 regularization\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB7o3zu1g6eT",
        "colab_type": "code",
        "outputId": "7d92b74e-bd45-45d6-e4fc-141802a2386f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train_orig), (x_test, y_test_orig) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train_orig.squeeze(), num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test_orig.squeeze(), num_classes)\n",
        "\n",
        "#shape of x_train is (50000, 32, 32, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 8s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD-j7Pg2do1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Data pre-processing step where we first normalize each channel according to the means and std deviations for CIFAR-10 examples.\n",
        "The means for red, green and blue are (125.3, 123, 113.9). The std deviations for red, green, blue are (63, 62.1, 66.7)\n",
        "Then use the data-augmentation techniques for the training set.\n",
        "\n",
        "1.) First use \"Random Cropping\" of images. For this, first pad 4 zeros on all 4 sides, i.e. top and bottom, right and left.\n",
        "    This will change our original CIFAR-10 images from \"32 x 32 x 3\" to \"40 x 40 x 3\". After this we randomly do cropping for \n",
        "    each image, such that we get back the same size i.e. \"32 x 32 x 3\".\n",
        "\n",
        "2.) Then do horizontal flip on half of the images obtained above to get mirrored images.\n",
        "\n",
        "While doing the data augmentation, the original no.of examples are 50k, so if we do data augmentation with factor of 2, then total images\n",
        "will be 1M. But this directly cannot be stored in array on Colab RAM. So I have created separate datasets of smaller size which will be dynamically changed during\n",
        "training. I have used data augmentation factor of 3 and created 4 total sets of data. Each dataset is of size 75K, such that 50K are original images and\n",
        "25K are augmented images(among 25K, all 25K are randomly cropped, and 12K are horizontally flipped)\n",
        "\"\"\"\n",
        "\n",
        "# Normalize each channel i.e red, green, blue\n",
        "def normalize_data(x_train):\n",
        "  \n",
        "  #first for red channel\n",
        "  x_train[:,:,:,0] -= 125.3\n",
        "  x_train[:,:,:,0] /= 63.0\n",
        "  \n",
        "  #green\n",
        "  x_train[:,:,:,1] -= 123\n",
        "  x_train[:,:,:,1] /= 62.1\n",
        "  \n",
        "  #blue\n",
        "  x_train[:,:,:,2] -= 113.9\n",
        "  x_train[:,:,:,2] /= 66.7\n",
        "  \n",
        "  return x_train\n",
        "\n",
        "\n",
        "# Following function does both, \"Random Cropping\" and \"horizontal flip\" on the input \"X_train\". It returns 2 datasets of size 75K each. In each dataset,\n",
        "# 50K are original images and 25K are randomly cropped images. In 25K, 12K are horizontally flipped images. In the second returned dataset, again 50K original\n",
        "# images, but the 25K images will be the second half which were not included in dataset1. Again randomly cropped and 12K flipped horizontally.\n",
        "def augment_data(x_train, y_train):\n",
        "  \n",
        "  # We need to first pad 4 zeros on all 4 sides for all images. We can achieve this by simply\n",
        "  # making a zero_matrix1 of all zeros of size (25000, 40, 40, 3) where 25000 are randomly cropped no.of examples we want to create.\n",
        "  # Then simply insert our 1st half CIFAR-10 images (25000, 32, 32, 3) in-between, such that they are at the center of \n",
        "  # our original zero_matrix1 which we created with all zeros. So by doing this, we automatically get matrix with images, but\n",
        "  # also the padding on all 4 sides.\n",
        "  # Similarly create zero_matrix2 with all zeros(25, 40, 40, 3). But now insert the second half of CIFAR-10 images i.e. 25K to 50K (25000, 32, 32, 3) in\n",
        "  # between zero_matrix2 to create padding.\n",
        "  \n",
        "  zero_matrix1 = np.zeros([25000, 40, 40, 3])  # all zeros matrix\n",
        "  zero_matrix2 = np.zeros([25000, 40, 40, 3])\n",
        "  \n",
        "  temp1 = np.zeros([25000, 32, 32, 3]) # to store images after doing random cropping (1st half of CIFAR-10)\n",
        "  temp2 = np.zeros([25000, 32, 32, 3]) # to store images after doing random cropping (2nd half of CIFAR-10)\n",
        " \n",
        "  \n",
        "  zero_matrix1[:, 4:36, 4:36, :] = x_train[0:25000,:,:,:]       # inserts the 1st half of images of x_train at the center of our zero_matrix1. \n",
        "                                                                # So this completes padding.\n",
        "  zero_matrix2[:, 4:36, 4:36, :] = x_train[25000:50000,:,:,:]   # inserts the 2nd half of images of x_train at the center of our zero_matrix2.\n",
        "                                                                # So this completes padding\n",
        "  \n",
        " \n",
        "  new_height = 40      # due to padding\n",
        "  new_width = 40       # due to padding\n",
        "  \n",
        "  required_size = 32\n",
        "  \n",
        "  # Loop over each example and crop to 32 x 32 x 3. This will fill the \"temp1\" and \"temp2\" with 1st half and 2nd half of X_train, but\n",
        "  # are randomly cropped.\n",
        "  for i in range(25000):\n",
        "    x_value1 = np.random.randint(0, (new_height - required_size) + 1)      # We need rand int between [ 0,9 )\n",
        "    y_value1 = np.random.randint(0, (new_width - required_size) + 1)       # We need rand int between [ 0,9 )\n",
        " \n",
        "    x_value2 = np.random.randint(0, (new_height - required_size) + 1)      # We need rand int between [ 0,9 )\n",
        "    y_value2 = np.random.randint(0, (new_width - required_size) + 1)       # We need rand int between [ 0,9 )\n",
        "    \n",
        "    temp1[i,:,:,:] = zero_matrix1[i, x_value1:(x_value1 + required_size), y_value1:(y_value1 + required_size), :] \n",
        "    temp2[i,:,:,:] = zero_matrix2[i, x_value2:(x_value2 + required_size), y_value2:(y_value2 + required_size), :]\n",
        "  \n",
        "  \n",
        "  # Once we get the randomly cropped images in temp1 and temp2, now horizonally flip 12K images in both temp1 and temp2. Randomly\n",
        "  # select images for both which decides which image to flip.\n",
        "  for i in range(12000):\n",
        "    random_img_1 = np.random.randint(0,25000)\n",
        "    random_img_2 = np.random.randint(0,25000)\n",
        "    \n",
        "    temp1[random_img_1,:,:,:] =  np.flip(temp1[random_img_1], axis=1)\n",
        "    temp2[random_img_2,:,:,:] =  np.flip(temp2[random_img_2], axis=1)     \n",
        "  \n",
        "  \n",
        "  \n",
        "  # Now concatenate original x_train with both temp1 and temp2 and return 2 augmented data sets.\n",
        "  return np.concatenate((x_train,temp1), axis=0), np.concatenate((x_train,temp2), axis=0)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZiucAZ4oxP7",
        "colab_type": "code",
        "outputId": "001b0ed2-a793-4f56-80ec-e4458cc4ed62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# First normalize both train and test set\n",
        "x_train = normalize_data(x_train)\n",
        "x_test = normalize_data(x_test)\n",
        "\n",
        "# Do data augmentation twice, because I'm using 4 total sets, which we will dynamically change during training.\n",
        "augmented_data_1, augmented_data_2 = random_crop(x_train, y_train)\n",
        "augmented_data_3, augmented_data_4 = random_crop(x_train, y_train)\n",
        "\n",
        "# As our augmented_data length is 75K for all 4 sets, the training labels length should also be 75K with appropriate labels.\n",
        "# So here y_train_aug_1 will work for both \"augmented_data_1\" and \"augmented_data_2\" sets. \n",
        "# y_train_aug_2 will work for both \"augmented_data_3\" and \"augmented_data_4\" sets.\n",
        "y_train_aug_1 = np.concatenate((y_train[0:50000], y_train[0:25000]), axis=0)\n",
        "y_train_aug_2 = np.concatenate((y_train[0:50000], y_train[25000:50000]), axis=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYqZsruIsXsE",
        "colab_type": "code",
        "outputId": "970770d7-2aa0-4eaf-b2dd-94caf74f1fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(augmented_data_1.shape)\n",
        "print(augmented_data_2.shape)\n",
        "print(augmented_data_3.shape)\n",
        "print(augmented_data_4.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(75000, 32, 32, 3)\n",
            "(75000, 32, 32, 3)\n",
            "(75000, 32, 32, 3)\n",
            "(75000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee-sge5Kg7vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, growth_rate):\n",
        "    #global compression \n",
        "    temp = input\n",
        "    for _ in range(num_layers):\n",
        "        #Using concept of bottle-neck i.e. dense-net-B. Batch_norm -> Relu -> Conv(1x1) -> Batch_norm -> Relu -> Conv(3x3)\n",
        "        BatchNorm1 = BatchNormalization(gamma_regularizer=l2(l2_reg), beta_regularizer=l2(l2_reg))(temp)\n",
        "        relu1 = Activation('relu')(BatchNorm1)\n",
        "        Conv2D_1_1 = Conv2D(4*growth_rate, (1,1), use_bias=False ,padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(relu1)\n",
        "        \n",
        "        BatchNorm2 = BatchNormalization(gamma_regularizer=l2(l2_reg), beta_regularizer=l2(l2_reg))(Conv2D_1_1)\n",
        "        relu2 = Activation(\"relu\")(BatchNorm2)\n",
        "        Conv2D_3_3 = Conv2D(growth_rate, (3,3), use_bias=False, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(relu2)\n",
        "        \n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOP6IPsGhBwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_transition(input, num_channels):\n",
        "    global compression\n",
        "    new_num_channels = int(compression * num_channels)   #Using concept of compression i.e. dense-net-C. To reduce no.of channels\n",
        "    \n",
        "    BatchNorm = BatchNormalization(gamma_regularizer=l2(l2_reg), beta_regularizer=l2(l2_reg))(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(new_num_channels, (1,1), use_bias=False, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(relu)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RaKFpubhDIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    # The final dense layer was also replaced with a Conv2d layer as below.\n",
        "    Conv2D_layer = Conv2D(num_classes, (1,1), use_bias=False, padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(relu)\n",
        "    GlobalAvgPooling = GlobalAveragePooling2D()(Conv2D_layer)\n",
        "    output = Activation(\"softmax\")(GlobalAvgPooling)\n",
        "   \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anPCpQWhhGb7",
        "colab_type": "code",
        "outputId": "33ac99e4-129f-4101-86bd-ce7f0bb852df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "initial_channels = 24   # growth_rate*2 where growth_rate=12\n",
        "growth_rate = 12        # growth_rate is the no.of new channels produced at each dense-block\n",
        "\n",
        "\n",
        "\n",
        "input = Input(shape=(32, 32, 3,))\n",
        "First_Conv2D = Conv2D(initial_channels, (3,3), use_bias=False ,padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2(l2_reg))(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, growth_rate)\n",
        "num_of_channels_after_first = (growth_rate * num_layers) + initial_channels     # where 'num_layers' is 12 in each dense-block, so total concatenated channels will be\n",
        "                                                                                # (growth * num_layers) + initial_channels. This is true for each dense-block\n",
        "First_Transition = add_transition(First_Block, num_of_channels_after_first)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, growth_rate)\n",
        "num_of_channels_after_second = num_of_channels_after_first + (growth_rate * num_layers)  # This gets added up from previous num_of_channels\n",
        "\n",
        "Second_Transition = add_transition(Second_Block, num_of_channels_after_second)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, growth_rate)\n",
        "\n",
        "output = output_layer(Third_Block)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kFh7pdxhNtT",
        "colab_type": "code",
        "outputId": "ecccdf89-1604-493a-a426-52924165f1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9826
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 24)   648         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 48)   1152        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 12)   5184        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 36)   0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 36)   144         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 36)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 48)   1728        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 12)   5184        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 48)   2304        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 48)   192         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 12)   5184        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 60)   0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 60)   240         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 60)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 48)   2880        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 48)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 48)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 12)   5184        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 72)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 48)   3456        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 48)   192         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 48)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 12)   5184        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 84)   0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 84)   336         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 84)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 48)   4032        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 48)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 48)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 12)   5184        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 96)   384         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 96)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 48)   4608        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 48)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 48)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 12)   5184        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 108)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 108)  432         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 108)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 48)   5184        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 48)   192         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 48)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 12)   5184        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 120)  480         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 32, 32, 120)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 48)   5760        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 32, 32, 48)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 48)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 32, 32, 12)   5184        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 132)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 32, 32, 132)  528         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 32, 32, 132)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 48)   6336        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 32, 32, 48)   192         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 32, 32, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 32, 32, 12)   5184        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 144)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 32, 32, 144)  576         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 32, 32, 144)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 32, 32, 48)   6912        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 32, 32, 48)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 32, 32, 48)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 32, 32, 12)   5184        activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 156)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 32, 32, 156)  624         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 32, 32, 156)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 32, 32, 48)   7488        activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 32, 32, 48)   192         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 32, 32, 48)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 32, 32, 12)   5184        activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 168)  0           concatenate_11[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 32, 32, 168)  672         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 32, 32, 168)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 32, 32, 84)   14112       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 84)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 84)   336         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 84)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 48)   4032        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 48)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 48)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 12)   5184        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 96)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 16, 16, 96)   384         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 48)   4608        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 16, 16, 48)   192         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 16, 16, 48)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 12)   5184        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 108)  0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 16, 16, 108)  432         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 16, 16, 108)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 16, 16, 48)   5184        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 16, 16, 48)   192         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 16, 16, 48)   0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 16, 16, 12)   5184        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 120)  0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 16, 16, 120)  480         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 16, 16, 120)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 16, 16, 48)   5760        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 16, 16, 48)   192         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 16, 16, 48)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 12)   5184        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 132)  0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 16, 16, 132)  528         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 16, 16, 132)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 48)   6336        activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 16, 16, 48)   192         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 16, 16, 48)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 12)   5184        activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 144)  0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 16, 16, 144)  576         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 16, 16, 144)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 16, 16, 48)   6912        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 16, 16, 48)   192         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 16, 16, 48)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 16, 16, 12)   5184        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 156)  0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 16, 16, 156)  624         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 16, 16, 156)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 16, 16, 48)   7488        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 16, 16, 48)   192         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 16, 16, 48)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 16, 16, 12)   5184        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 168)  0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 16, 16, 168)  672         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 16, 16, 168)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 16, 16, 48)   8064        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 16, 16, 48)   192         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 16, 16, 48)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 16, 16, 12)   5184        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 180)  0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 16, 16, 180)  720         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 16, 16, 180)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 16, 16, 48)   8640        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 16, 16, 48)   192         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 16, 16, 48)   0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 16, 16, 12)   5184        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 192)  0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 16, 16, 192)  768         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 16, 16, 192)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 16, 16, 48)   9216        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 48)   192         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 48)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 12)   5184        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 204)  0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 204)  816         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 204)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 48)   9792        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 48)   192         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 48)   0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 12)   5184        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 216)  0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 16, 16, 216)  864         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 16, 16, 216)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 48)   10368       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 16, 16, 48)   192         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 16, 16, 48)   0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 12)   5184        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 228)  0           concatenate_23[0][0]             \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 16, 16, 228)  912         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 16, 16, 228)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 156)  35568       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 156)    0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 8, 8, 156)    624         average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 8, 8, 156)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 8, 8, 48)     7488        activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 8, 8, 48)     192         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 8, 8, 48)     0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 8, 8, 12)     5184        activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 168)    0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 8, 8, 168)    672         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 8, 8, 168)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 8, 8, 48)     8064        activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 48)     192         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 48)     0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 12)     5184        activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 180)    0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 180)    720         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 180)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 48)     8640        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 48)     192         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 48)     0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 12)     5184        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 192)    0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 192)    768         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 8, 8, 192)    0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 48)     9216        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 8, 8, 48)     192         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 8, 8, 48)     0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 12)     5184        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 204)    0           concatenate_27[0][0]             \n",
            "                                                                 conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 8, 8, 204)    816         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 8, 8, 204)    0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 48)     9792        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 8, 8, 48)     192         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 8, 8, 48)     0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 12)     5184        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 216)    0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 8, 8, 216)    864         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 8, 8, 216)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 48)     10368       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 8, 8, 48)     192         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 8, 8, 48)     0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 8, 8, 12)     5184        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 228)    0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 8, 8, 228)    912         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 8, 8, 228)    0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 8, 8, 48)     10944       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 8, 8, 48)     192         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 8, 8, 48)     0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 8, 8, 12)     5184        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 240)    0           concatenate_30[0][0]             \n",
            "                                                                 conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 8, 8, 240)    960         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 8, 8, 240)    0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 8, 8, 48)     11520       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 8, 8, 48)     192         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 8, 8, 48)     0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 8, 8, 12)     5184        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 252)    0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 8, 8, 252)    1008        concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 8, 8, 252)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 8, 8, 48)     12096       activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 8, 8, 48)     192         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 8, 8, 48)     0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 8, 8, 12)     5184        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 264)    0           concatenate_32[0][0]             \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 8, 8, 264)    1056        concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 8, 8, 264)    0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 8, 8, 48)     12672       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 8, 8, 48)     192         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 8, 8, 48)     0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 8, 8, 12)     5184        activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 276)    0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 8, 8, 276)    1104        concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 8, 8, 276)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 8, 8, 48)     13248       activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 8, 8, 48)     192         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 8, 8, 48)     0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 8, 8, 12)     5184        activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 288)    0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 8, 8, 288)    1152        concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 8, 8, 288)    0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 8, 8, 48)     13824       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 8, 8, 48)     192         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 8, 8, 48)     0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 8, 8, 12)     5184        activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 300)    0           concatenate_35[0][0]             \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 300)    1200        concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 300)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 8, 8, 10)     3000        activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 10)           0           conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 10)           0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 537,936\n",
            "Trainable params: 522,000\n",
            "Non-trainable params: 15,936\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XOsW3ahSkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class is made so that it can inherit from \"Sequence\" class, so that we override functions like \"getitem\" and \"on_epoch_end\" to get batches\n",
        "# of data during training. \"on_epoch_end\" is responsible for changing the dataset dynamically after end of each epoch. As we have created 4 total augmented data\n",
        "# sets, \"on_epoch_end\" continuously keeps using datasets one after another after each epoch. It keeps track of which dataset was used in previous epoch, \n",
        "# and based on that goes to the next data-set in the next epoch.\n",
        "# The object of this class will be passed to \"fit_generator\" for training.\n",
        "\n",
        "class CIFAR10Sequence(Sequence):\n",
        "\n",
        "    def __init__(self, x_set, y_set, batch_size, last_aug_index):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.last_aug_data = last_aug_index\n",
        "        self.epoch = 0\n",
        "        \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return batch_x, batch_y\n",
        "      \n",
        "    def on_epoch_end(self):\n",
        "        if (self.last_aug_data == 1):\n",
        "          self.x = augmented_data_2\n",
        "          self.y = y_train_aug_2\n",
        "          self.last_aug_data += 1\n",
        "        elif (self.last_aug_data == 2):\n",
        "          self.x = augmented_data_3\n",
        "          self.y = y_train_aug_1\n",
        "          self.last_aug_data += 1\n",
        "        elif (self.last_aug_data == 3):\n",
        "          self.x = augmented_data_4\n",
        "          self.y = y_train_aug_2\n",
        "          self.last_aug_data += 1\n",
        "        elif (self.last_aug_data == 4):\n",
        "          self.x = augmented_data_1\n",
        "          self.y = y_train_aug_1\n",
        "          self.last_aug_data = 1\n",
        "          \n",
        "        self.epoch += 1   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuaMMSmezWwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback functions to be used, \"ModelCheckPoint\" and \"LearningRateScheduler\".\n",
        "# ModelCheckPoint to save our model weights after every epoch. This help during system failure during training, to re-train from last saved weights\n",
        "# LearningRateScheduler to change our initial learning_rate which is 0.1. After 50% of epochs, divide by 10, again after 75% of epochs, again divide by 10.\n",
        "\n",
        "# This is the folder inside google drive where weights will be saved. Weights will be saved after each epoch.\n",
        "filepath = \"/content/drive/My Drive/DensenetCheckpoints4/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"                                        \n",
        "\n",
        "callback1 = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)         \n",
        "\n",
        "# Schedule function which is used by \"LearningRateScheduler\". This takes in epoch index and current learning rate as input\n",
        "def schedule(epoch, learning_rate):\n",
        "  if ((epoch == 124) or (epoch == 186)):       # i.e. 50% of 250 wil be 125, but epoch starts from zero, so take 124 and 75% will be 186\n",
        "    learning_rate = learning_rate / 10\n",
        "  \n",
        "  return learning_rate\n",
        "\n",
        "callback2 = LearningRateScheduler(schedule, verbose=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crhGk7kEhXAz",
        "colab_type": "code",
        "outputId": "54eb730e-aa7d-4cd8-b052-1d8e29de07fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1788
        }
      },
      "source": [
        "# Object of CIFA10Sequence class. Start with dataset1 and dataset2.\n",
        "training_generator_seq = CIFAR10Sequence(augmented_data_1, y_train_aug_1, batch_size, 1)\n",
        "\n",
        "\n",
        "\n",
        "# Determine loss-function and optimizer\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=SGD(learning_rate, momentum=0.9, nesterov=True),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# start training using fit_generator.\n",
        "model.fit_generator(generator=training_generator_seq,\n",
        "                    steps_per_epoch=m.ceil(augmented_data_1.shape[0]/ batch_size),\n",
        "                    epochs=250,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=6,\n",
        "                    verbose=1,\n",
        "                    callbacks=[callback1, callback2])\n",
        "\n",
        "\n",
        "model.save(\"/content/drive/My Drive/DensenetCheckpoints4/FinalModel/model.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/250\n",
            "586/586 [==============================] - 365s 623ms/step - loss: 2.2519 - acc: 0.6022 - val_loss: 1.9314 - val_acc: 0.6846\n",
            "Epoch 2/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 1.6125 - acc: 0.7723 - val_loss: 1.7628 - val_acc: 0.6804\n",
            "Epoch 3/250\n",
            "586/586 [==============================] - 345s 588ms/step - loss: 1.3110 - acc: 0.8258 - val_loss: 1.7205 - val_acc: 0.6672\n",
            "Epoch 4/250\n",
            "586/586 [==============================] - 344s 586ms/step - loss: 1.1133 - acc: 0.8523 - val_loss: 1.2840 - val_acc: 0.7698\n",
            "Epoch 5/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.9666 - acc: 0.8703 - val_loss: 1.2903 - val_acc: 0.7687\n",
            "Epoch 6/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.8597 - acc: 0.8816 - val_loss: 1.0520 - val_acc: 0.8071\n",
            "Epoch 7/250\n",
            "586/586 [==============================] - 344s 586ms/step - loss: 0.7781 - acc: 0.8906 - val_loss: 1.3748 - val_acc: 0.7069\n",
            "Epoch 8/250\n",
            "586/586 [==============================] - 344s 586ms/step - loss: 0.7133 - acc: 0.8993 - val_loss: 1.0318 - val_acc: 0.7868\n",
            "Epoch 9/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.6660 - acc: 0.9036 - val_loss: 0.9225 - val_acc: 0.8237\n",
            "Epoch 10/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.6322 - acc: 0.9057 - val_loss: 1.0202 - val_acc: 0.7936\n",
            "Epoch 11/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.6068 - acc: 0.9098 - val_loss: 0.8623 - val_acc: 0.8301\n",
            "Epoch 12/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.5858 - acc: 0.9114 - val_loss: 0.8536 - val_acc: 0.8298\n",
            "Epoch 13/250\n",
            "586/586 [==============================] - 344s 586ms/step - loss: 0.5664 - acc: 0.9151 - val_loss: 1.1941 - val_acc: 0.7703\n",
            "Epoch 14/250\n",
            "586/586 [==============================] - 344s 586ms/step - loss: 0.5565 - acc: 0.9152 - val_loss: 0.9020 - val_acc: 0.8006\n",
            "Epoch 15/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.5386 - acc: 0.9198 - val_loss: 1.0151 - val_acc: 0.7907\n",
            "Epoch 16/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.5310 - acc: 0.9204 - val_loss: 0.8113 - val_acc: 0.8331\n",
            "Epoch 17/250\n",
            "586/586 [==============================] - 345s 589ms/step - loss: 0.5291 - acc: 0.9206 - val_loss: 0.8228 - val_acc: 0.8339\n",
            "Epoch 18/250\n",
            "586/586 [==============================] - 345s 588ms/step - loss: 0.5178 - acc: 0.9235 - val_loss: 1.1808 - val_acc: 0.7637\n",
            "Epoch 19/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.5155 - acc: 0.9233 - val_loss: 1.5071 - val_acc: 0.6959\n",
            "Epoch 20/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.5103 - acc: 0.9249 - val_loss: 1.0085 - val_acc: 0.7929\n",
            "Epoch 21/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.5071 - acc: 0.9249 - val_loss: 1.2987 - val_acc: 0.7496\n",
            "Epoch 22/250\n",
            "586/586 [==============================] - 344s 586ms/step - loss: 0.5020 - acc: 0.9257 - val_loss: 0.9836 - val_acc: 0.8117\n",
            "Epoch 23/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.4987 - acc: 0.9284 - val_loss: 1.0820 - val_acc: 0.7929\n",
            "Epoch 24/250\n",
            "586/586 [==============================] - 344s 588ms/step - loss: 0.4943 - acc: 0.9288 - val_loss: 1.2500 - val_acc: 0.7360\n",
            "Epoch 25/250\n",
            "586/586 [==============================] - 345s 589ms/step - loss: 0.4936 - acc: 0.9298 - val_loss: 0.6592 - val_acc: 0.8769\n",
            "Epoch 26/250\n",
            "586/586 [==============================] - 345s 589ms/step - loss: 0.4861 - acc: 0.9314 - val_loss: 0.9373 - val_acc: 0.8116\n",
            "Epoch 27/250\n",
            "586/586 [==============================] - 344s 588ms/step - loss: 0.4848 - acc: 0.9316 - val_loss: 0.7522 - val_acc: 0.8498\n",
            "Epoch 28/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.4890 - acc: 0.9307 - val_loss: 0.8611 - val_acc: 0.8257\n",
            "Epoch 29/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.4820 - acc: 0.9323 - val_loss: 0.7719 - val_acc: 0.8477\n",
            "Epoch 30/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4811 - acc: 0.9325 - val_loss: 0.8902 - val_acc: 0.8117\n",
            "Epoch 31/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.4810 - acc: 0.9336 - val_loss: 1.1468 - val_acc: 0.7811\n",
            "Epoch 32/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4765 - acc: 0.9344 - val_loss: 0.9147 - val_acc: 0.8225\n",
            "Epoch 33/250\n",
            "586/586 [==============================] - 344s 588ms/step - loss: 0.4723 - acc: 0.9352 - val_loss: 0.8614 - val_acc: 0.8275\n",
            "Epoch 34/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.4716 - acc: 0.9362 - val_loss: 0.9645 - val_acc: 0.8064\n",
            "Epoch 35/250\n",
            "586/586 [==============================] - 344s 587ms/step - loss: 0.4703 - acc: 0.9363 - val_loss: 1.1392 - val_acc: 0.7563\n",
            "Epoch 36/250\n",
            "586/586 [==============================] - 346s 590ms/step - loss: 0.4704 - acc: 0.9364 - val_loss: 0.9234 - val_acc: 0.8202\n",
            "Epoch 37/250\n",
            "586/586 [==============================] - 347s 592ms/step - loss: 0.4718 - acc: 0.9365 - val_loss: 0.8839 - val_acc: 0.8269\n",
            "Epoch 38/250\n",
            "586/586 [==============================] - 346s 590ms/step - loss: 0.4598 - acc: 0.9393 - val_loss: 0.8966 - val_acc: 0.8083\n",
            "Epoch 39/250\n",
            "586/586 [==============================] - 346s 591ms/step - loss: 0.4706 - acc: 0.9358 - val_loss: 0.9051 - val_acc: 0.8304\n",
            "Epoch 40/250\n",
            "586/586 [==============================] - 346s 591ms/step - loss: 0.4672 - acc: 0.9377 - val_loss: 1.1944 - val_acc: 0.7910\n",
            "Epoch 41/250\n",
            "586/586 [==============================] - 346s 590ms/step - loss: 0.4650 - acc: 0.9383 - val_loss: 0.8925 - val_acc: 0.8339\n",
            "Epoch 42/250\n",
            "586/586 [==============================] - 346s 590ms/step - loss: 0.4598 - acc: 0.9388 - val_loss: 0.8309 - val_acc: 0.8332\n",
            "Epoch 43/250\n",
            "586/586 [==============================] - 346s 590ms/step - loss: 0.4639 - acc: 0.9381 - val_loss: 1.4165 - val_acc: 0.7171\n",
            "Epoch 44/250\n",
            "586/586 [==============================] - 346s 591ms/step - loss: 0.4569 - acc: 0.9402 - val_loss: 0.8449 - val_acc: 0.8370\n",
            "Epoch 45/250\n",
            "586/586 [==============================] - 346s 591ms/step - loss: 0.4596 - acc: 0.9387 - val_loss: 1.6348 - val_acc: 0.7330\n",
            "Epoch 46/250\n",
            "586/586 [==============================] - 346s 591ms/step - loss: 0.4575 - acc: 0.9401 - val_loss: 1.6482 - val_acc: 0.6992\n",
            "Epoch 47/250\n",
            "586/586 [==============================] - 346s 591ms/step - loss: 0.4620 - acc: 0.9389 - val_loss: 1.0608 - val_acc: 0.7831\n",
            "Epoch 48/250\n",
            "586/586 [==============================] - 346s 590ms/step - loss: 0.4549 - acc: 0.9401 - val_loss: 1.1748 - val_acc: 0.7635\n",
            "Epoch 49/250\n",
            "586/586 [==============================] - 345s 589ms/step - loss: 0.4582 - acc: 0.9404 - val_loss: 0.7720 - val_acc: 0.8442\n",
            "Epoch 50/250\n",
            "104/586 [====>.........................] - ETA: 4:31 - loss: 0.4490 - acc: 0.9409"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwIERDPi3keL",
        "colab_type": "code",
        "outputId": "c6a6ec00-a0b1-43c2-fc3f-1aaebdc0fd50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4423
        }
      },
      "source": [
        "# Restart training from epoch 50.\n",
        "\n",
        "training_generator_seq = CIFAR10Sequence(augmented_data_1, y_train_aug_1, batch_size, 1)\n",
        "\n",
        "model = load_model(\"/content/drive/My Drive/DensenetCheckpoints4/weights-improvement-49-0.84.hdf5\")\n",
        "\n",
        "\n",
        "\n",
        "model.fit_generator(training_generator_seq,\n",
        "                    steps_per_epoch=m.ceil(augmented_data_1.shape[0] / batch_size), epochs=250, validation_data=(x_test, y_test), verbose=1, callbacks=[callback1, callback2], initial_epoch=49)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 50/250\n",
            "586/586 [==============================] - 356s 607ms/step - loss: 0.4597 - acc: 0.9397 - val_loss: 0.9728 - val_acc: 0.8112\n",
            "Epoch 51/250\n",
            "586/586 [==============================] - 341s 583ms/step - loss: 0.4597 - acc: 0.9387 - val_loss: 1.1768 - val_acc: 0.7792\n",
            "Epoch 52/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.4606 - acc: 0.9396 - val_loss: 0.9109 - val_acc: 0.8351\n",
            "Epoch 53/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4561 - acc: 0.9405 - val_loss: 1.5151 - val_acc: 0.7126\n",
            "Epoch 54/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4497 - acc: 0.9416 - val_loss: 0.9957 - val_acc: 0.8142\n",
            "Epoch 55/250\n",
            "586/586 [==============================] - 341s 583ms/step - loss: 0.4489 - acc: 0.9408 - val_loss: 1.2063 - val_acc: 0.7781\n",
            "Epoch 56/250\n",
            "586/586 [==============================] - 341s 583ms/step - loss: 0.4544 - acc: 0.9393 - val_loss: 1.1330 - val_acc: 0.7760\n",
            "Epoch 57/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4471 - acc: 0.9425 - val_loss: 1.0222 - val_acc: 0.7895\n",
            "Epoch 58/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4504 - acc: 0.9413 - val_loss: 0.9053 - val_acc: 0.8279\n",
            "Epoch 59/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4477 - acc: 0.9416 - val_loss: 0.8259 - val_acc: 0.8418\n",
            "Epoch 60/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4467 - acc: 0.9437 - val_loss: 1.6028 - val_acc: 0.6932\n",
            "Epoch 61/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4416 - acc: 0.9438 - val_loss: 1.0111 - val_acc: 0.8019\n",
            "Epoch 62/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4497 - acc: 0.9417 - val_loss: 0.8858 - val_acc: 0.8243\n",
            "Epoch 63/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4407 - acc: 0.9444 - val_loss: 1.4771 - val_acc: 0.7317\n",
            "Epoch 64/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4486 - acc: 0.9417 - val_loss: 1.1837 - val_acc: 0.7653\n",
            "Epoch 65/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4426 - acc: 0.9438 - val_loss: 0.7670 - val_acc: 0.8433\n",
            "Epoch 66/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4425 - acc: 0.9439 - val_loss: 2.1136 - val_acc: 0.6618\n",
            "Epoch 67/250\n",
            "586/586 [==============================] - 344s 586ms/step - loss: 0.4407 - acc: 0.9442 - val_loss: 1.0246 - val_acc: 0.7956\n",
            "Epoch 68/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4416 - acc: 0.9442 - val_loss: 0.8519 - val_acc: 0.8297\n",
            "Epoch 69/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4351 - acc: 0.9460 - val_loss: 0.9053 - val_acc: 0.8237\n",
            "Epoch 70/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4407 - acc: 0.9434 - val_loss: 0.7243 - val_acc: 0.8614\n",
            "Epoch 71/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4340 - acc: 0.9460 - val_loss: 0.9694 - val_acc: 0.8110\n",
            "Epoch 72/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4366 - acc: 0.9456 - val_loss: 1.0812 - val_acc: 0.8122\n",
            "Epoch 73/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4382 - acc: 0.9436 - val_loss: 0.9132 - val_acc: 0.8128\n",
            "Epoch 74/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4350 - acc: 0.9452 - val_loss: 1.0470 - val_acc: 0.8073\n",
            "Epoch 75/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4355 - acc: 0.9445 - val_loss: 1.1605 - val_acc: 0.7914\n",
            "Epoch 76/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.4368 - acc: 0.9454 - val_loss: 0.8461 - val_acc: 0.8425\n",
            "Epoch 77/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4333 - acc: 0.9448 - val_loss: 0.7666 - val_acc: 0.8538\n",
            "Epoch 78/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4367 - acc: 0.9447 - val_loss: 0.7855 - val_acc: 0.8514\n",
            "Epoch 79/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4302 - acc: 0.9473 - val_loss: 0.9680 - val_acc: 0.8205\n",
            "Epoch 80/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4339 - acc: 0.9463 - val_loss: 1.0481 - val_acc: 0.7890\n",
            "Epoch 81/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.4278 - acc: 0.9473 - val_loss: 1.0731 - val_acc: 0.8184\n",
            "Epoch 82/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4352 - acc: 0.9450 - val_loss: 0.9710 - val_acc: 0.8120\n",
            "Epoch 83/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4238 - acc: 0.9481 - val_loss: 0.8310 - val_acc: 0.8357\n",
            "Epoch 84/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4338 - acc: 0.9451 - val_loss: 1.3782 - val_acc: 0.7390\n",
            "Epoch 85/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4256 - acc: 0.9477 - val_loss: 0.9123 - val_acc: 0.8200\n",
            "Epoch 86/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.4241 - acc: 0.9480 - val_loss: 0.8314 - val_acc: 0.8364\n",
            "Epoch 87/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4291 - acc: 0.9464 - val_loss: 0.8658 - val_acc: 0.8415\n",
            "Epoch 88/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4283 - acc: 0.9469 - val_loss: 1.0944 - val_acc: 0.8026\n",
            "Epoch 89/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4261 - acc: 0.9471 - val_loss: 0.7774 - val_acc: 0.8555\n",
            "Epoch 90/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4257 - acc: 0.9477 - val_loss: 0.9018 - val_acc: 0.8283\n",
            "Epoch 91/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4195 - acc: 0.9495 - val_loss: 1.2776 - val_acc: 0.7652\n",
            "Epoch 92/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4259 - acc: 0.9480 - val_loss: 1.1138 - val_acc: 0.7704\n",
            "Epoch 93/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4252 - acc: 0.9474 - val_loss: 0.9498 - val_acc: 0.8254\n",
            "Epoch 94/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4227 - acc: 0.9490 - val_loss: 0.9262 - val_acc: 0.8182\n",
            "Epoch 95/250\n",
            "586/586 [==============================] - 339s 578ms/step - loss: 0.4223 - acc: 0.9490 - val_loss: 0.7469 - val_acc: 0.8610\n",
            "Epoch 96/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4242 - acc: 0.9477 - val_loss: 0.8678 - val_acc: 0.8304\n",
            "Epoch 97/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.4254 - acc: 0.9479 - val_loss: 1.1120 - val_acc: 0.7801\n",
            "Epoch 98/250\n",
            "586/586 [==============================] - 341s 583ms/step - loss: 0.4267 - acc: 0.9473 - val_loss: 1.1391 - val_acc: 0.7922\n",
            "Epoch 99/250\n",
            "586/586 [==============================] - 341s 583ms/step - loss: 0.4195 - acc: 0.9483 - val_loss: 0.8735 - val_acc: 0.8385\n",
            "Epoch 100/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.4189 - acc: 0.9496 - val_loss: 0.7245 - val_acc: 0.8567\n",
            "Epoch 101/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.4193 - acc: 0.9489 - val_loss: 0.9140 - val_acc: 0.8183\n",
            "Epoch 102/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4283 - acc: 0.9457 - val_loss: 0.7715 - val_acc: 0.8594\n",
            "Epoch 103/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.4153 - acc: 0.9508 - val_loss: 1.3506 - val_acc: 0.7542\n",
            "Epoch 104/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4213 - acc: 0.9483 - val_loss: 0.9558 - val_acc: 0.8191\n",
            "Epoch 105/250\n",
            "586/586 [==============================] - 340s 581ms/step - loss: 0.4185 - acc: 0.9494 - val_loss: 0.7788 - val_acc: 0.8549\n",
            "Epoch 106/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4235 - acc: 0.9467 - val_loss: 0.7221 - val_acc: 0.8641\n",
            "Epoch 107/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4214 - acc: 0.9483 - val_loss: 0.7718 - val_acc: 0.8541\n",
            "Epoch 108/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4184 - acc: 0.9496 - val_loss: 0.7588 - val_acc: 0.8599\n",
            "Epoch 109/250\n",
            "586/586 [==============================] - 340s 579ms/step - loss: 0.4200 - acc: 0.9493 - val_loss: 1.0811 - val_acc: 0.7880\n",
            "Epoch 110/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4190 - acc: 0.9497 - val_loss: 0.8229 - val_acc: 0.8493\n",
            "Epoch 111/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4182 - acc: 0.9499 - val_loss: 0.8368 - val_acc: 0.8363\n",
            "Epoch 112/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4198 - acc: 0.9488 - val_loss: 0.7968 - val_acc: 0.8451\n",
            "Epoch 113/250\n",
            "586/586 [==============================] - 339s 579ms/step - loss: 0.4190 - acc: 0.9486 - val_loss: 1.0555 - val_acc: 0.8161\n",
            "Epoch 114/250\n",
            "586/586 [==============================] - 339s 579ms/step - loss: 0.4191 - acc: 0.9491 - val_loss: 0.8618 - val_acc: 0.8428\n",
            "Epoch 115/250\n",
            "586/586 [==============================] - 341s 581ms/step - loss: 0.4142 - acc: 0.9506 - val_loss: 0.8799 - val_acc: 0.8343\n",
            "Epoch 116/250\n",
            "586/586 [==============================] - 339s 578ms/step - loss: 0.4238 - acc: 0.9477 - val_loss: 0.9242 - val_acc: 0.8231\n",
            "Epoch 117/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4172 - acc: 0.9506 - val_loss: 0.7859 - val_acc: 0.8570\n",
            "Epoch 118/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4186 - acc: 0.9494 - val_loss: 1.1908 - val_acc: 0.7899\n",
            "Epoch 119/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4171 - acc: 0.9498 - val_loss: 1.3573 - val_acc: 0.7758\n",
            "Epoch 120/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.4160 - acc: 0.9499 - val_loss: 0.9779 - val_acc: 0.8164\n",
            "Epoch 121/250\n",
            "586/586 [==============================] - 341s 581ms/step - loss: 0.4117 - acc: 0.9512 - val_loss: 0.9382 - val_acc: 0.8009\n",
            "Epoch 122/250\n",
            "586/586 [==============================] - 341s 581ms/step - loss: 0.4196 - acc: 0.9485 - val_loss: 0.7776 - val_acc: 0.8500\n",
            "Epoch 123/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.4111 - acc: 0.9516 - val_loss: 1.0136 - val_acc: 0.7808\n",
            "Epoch 124/250\n",
            "586/586 [==============================] - 341s 581ms/step - loss: 0.4159 - acc: 0.9504 - val_loss: 1.0309 - val_acc: 0.7937\n",
            "Epoch 125/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.3364 - acc: 0.9791 - val_loss: 0.4753 - val_acc: 0.9343\n",
            "Epoch 126/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2941 - acc: 0.9927 - val_loss: 0.4631 - val_acc: 0.9378\n",
            "Epoch 127/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2805 - acc: 0.9950 - val_loss: 0.4587 - val_acc: 0.9382\n",
            "Epoch 128/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2675 - acc: 0.9971 - val_loss: 0.4556 - val_acc: 0.9381\n",
            "Epoch 129/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2627 - acc: 0.9967 - val_loss: 0.4557 - val_acc: 0.9377\n",
            "Epoch 130/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2543 - acc: 0.9977 - val_loss: 0.4478 - val_acc: 0.9387\n",
            "Epoch 131/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2469 - acc: 0.9983 - val_loss: 0.4478 - val_acc: 0.9371\n",
            "Epoch 132/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2386 - acc: 0.9992 - val_loss: 0.4389 - val_acc: 0.9399\n",
            "Epoch 133/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.2343 - acc: 0.9989 - val_loss: 0.4391 - val_acc: 0.9386\n",
            "Epoch 134/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.2280 - acc: 0.9993 - val_loss: 0.4331 - val_acc: 0.9393\n",
            "Epoch 135/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.2220 - acc: 0.9994 - val_loss: 0.4287 - val_acc: 0.9391\n",
            "Epoch 136/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.2159 - acc: 0.9997 - val_loss: 0.4279 - val_acc: 0.9407\n",
            "Epoch 137/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.2115 - acc: 0.9998 - val_loss: 0.4255 - val_acc: 0.9388\n",
            "Epoch 138/250\n",
            "586/586 [==============================] - 343s 586ms/step - loss: 0.2064 - acc: 0.9998 - val_loss: 0.4235 - val_acc: 0.9405\n",
            "Epoch 139/250\n",
            "586/586 [==============================] - 343s 585ms/step - loss: 0.2012 - acc: 0.9999 - val_loss: 0.4187 - val_acc: 0.9400\n",
            "Epoch 140/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.1962 - acc: 0.9999 - val_loss: 0.4161 - val_acc: 0.9398\n",
            "Epoch 141/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.1921 - acc: 1.0000 - val_loss: 0.4120 - val_acc: 0.9393\n",
            "Epoch 142/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.1876 - acc: 1.0000 - val_loss: 0.4081 - val_acc: 0.9391\n",
            "Epoch 143/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.1831 - acc: 1.0000 - val_loss: 0.4057 - val_acc: 0.9393\n",
            "Epoch 144/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.1787 - acc: 1.0000 - val_loss: 0.4023 - val_acc: 0.9393\n",
            "Epoch 145/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.1749 - acc: 1.0000 - val_loss: 0.4007 - val_acc: 0.9393\n",
            "Epoch 146/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.1708 - acc: 1.0000 - val_loss: 0.3961 - val_acc: 0.9393\n",
            "Epoch 147/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.1668 - acc: 1.0000 - val_loss: 0.3959 - val_acc: 0.9393\n",
            "Epoch 148/250\n",
            "586/586 [==============================] - 342s 584ms/step - loss: 0.1629 - acc: 1.0000 - val_loss: 0.3904 - val_acc: 0.9396\n",
            "Epoch 149/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.1594 - acc: 1.0000 - val_loss: 0.3911 - val_acc: 0.9403\n",
            "Epoch 150/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.1557 - acc: 1.0000 - val_loss: 0.3844 - val_acc: 0.9386\n",
            "Epoch 151/250\n",
            "586/586 [==============================] - 340s 581ms/step - loss: 0.1521 - acc: 1.0000 - val_loss: 0.3830 - val_acc: 0.9396\n",
            "Epoch 152/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.1486 - acc: 1.0000 - val_loss: 0.3803 - val_acc: 0.9396\n",
            "Epoch 153/250\n",
            "586/586 [==============================] - 339s 579ms/step - loss: 0.1454 - acc: 1.0000 - val_loss: 0.3779 - val_acc: 0.9405\n",
            "Epoch 154/250\n",
            "586/586 [==============================] - 339s 578ms/step - loss: 0.1421 - acc: 1.0000 - val_loss: 0.3758 - val_acc: 0.9382\n",
            "Epoch 155/250\n",
            "586/586 [==============================] - 339s 579ms/step - loss: 0.1388 - acc: 1.0000 - val_loss: 0.3717 - val_acc: 0.9393\n",
            "Epoch 156/250\n",
            "586/586 [==============================] - 340s 581ms/step - loss: 0.1356 - acc: 1.0000 - val_loss: 0.3698 - val_acc: 0.9396\n",
            "Epoch 157/250\n",
            "586/586 [==============================] - 340s 581ms/step - loss: 0.1327 - acc: 1.0000 - val_loss: 0.3668 - val_acc: 0.9396\n",
            "Epoch 158/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.1296 - acc: 1.0000 - val_loss: 0.3665 - val_acc: 0.9385\n",
            "Epoch 159/250\n",
            "586/586 [==============================] - 341s 581ms/step - loss: 0.1267 - acc: 1.0000 - val_loss: 0.3631 - val_acc: 0.9385\n",
            "Epoch 160/250\n",
            "586/586 [==============================] - 340s 581ms/step - loss: 0.1237 - acc: 1.0000 - val_loss: 0.3613 - val_acc: 0.9383\n",
            "Epoch 161/250\n",
            "586/586 [==============================] - 340s 579ms/step - loss: 0.1211 - acc: 1.0000 - val_loss: 0.3604 - val_acc: 0.9386\n",
            "Epoch 162/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.1183 - acc: 1.0000 - val_loss: 0.3570 - val_acc: 0.9390\n",
            "Epoch 163/250\n",
            "586/586 [==============================] - 340s 581ms/step - loss: 0.1157 - acc: 1.0000 - val_loss: 0.3559 - val_acc: 0.9387\n",
            "Epoch 164/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.1130 - acc: 1.0000 - val_loss: 0.3546 - val_acc: 0.9384\n",
            "Epoch 165/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.1106 - acc: 1.0000 - val_loss: 0.3499 - val_acc: 0.9389\n",
            "Epoch 166/250\n",
            "586/586 [==============================] - 340s 580ms/step - loss: 0.1081 - acc: 1.0000 - val_loss: 0.3504 - val_acc: 0.9377\n",
            "Epoch 167/250\n",
            "586/586 [==============================] - 340s 579ms/step - loss: 0.1056 - acc: 1.0000 - val_loss: 0.3467 - val_acc: 0.9391\n",
            "Epoch 168/250\n",
            "586/586 [==============================] - 341s 583ms/step - loss: 0.1032 - acc: 1.0000 - val_loss: 0.3458 - val_acc: 0.9390\n",
            "Epoch 169/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.1010 - acc: 1.0000 - val_loss: 0.3463 - val_acc: 0.9385\n",
            "Epoch 170/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.0987 - acc: 1.0000 - val_loss: 0.3448 - val_acc: 0.9379\n",
            "Epoch 171/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.0965 - acc: 1.0000 - val_loss: 0.3399 - val_acc: 0.9384\n",
            "Epoch 172/250\n",
            "586/586 [==============================] - 341s 582ms/step - loss: 0.0943 - acc: 1.0000 - val_loss: 0.3403 - val_acc: 0.9381\n",
            "Epoch 173/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.0923 - acc: 1.0000 - val_loss: 0.3396 - val_acc: 0.9380\n",
            "Epoch 174/250\n",
            "586/586 [==============================] - 342s 583ms/step - loss: 0.0902 - acc: 1.0000 - val_loss: 0.3377 - val_acc: 0.9379\n",
            "Epoch 175/250\n",
            "507/586 [========================>.....] - ETA: 44s - loss: 0.0883 - acc: 1.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTUjbLIGefvl",
        "colab_type": "code",
        "outputId": "0f06af80-8f1b-49a1-fa04-9baa66c1b4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2652
        }
      },
      "source": [
        "# Restart training from epoch 175\n",
        "# dataset 2 was used because at 175 epoch dataset2 was supposed to be used according to on_epoch_end.\n",
        "training_generator_seq = CIFAR10Sequence(augmented_data_2, y_train_aug_2, batch_size, 2)\n",
        "\n",
        "model = load_model(\"/content/drive/My Drive/DensenetCheckpoints4/weights-improvement-174-0.94.hdf5\")\n",
        "\n",
        "\n",
        "model.fit_generator(training_generator_seq,\n",
        "                    steps_per_epoch=m.ceil(augmented_data_2.shape[0] / batch_size), epochs=250, validation_data=(x_test, y_test), verbose=1, callbacks=[callback1, callback2], initial_epoch=174, shuffle=True)\n",
        "\n",
        "\n",
        "# Save final model\n",
        "model.save(\"/content/drive/My Drive/DensenetCheckpoints4/FinalModel/model.hdf5\")\n",
        "\n",
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 175/250\n",
            "586/586 [==============================] - 370s 632ms/step - loss: 0.1187 - acc: 0.9902 - val_loss: 0.3838 - val_acc: 0.9194\n",
            "Epoch 176/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.1165 - acc: 0.9911 - val_loss: 0.3769 - val_acc: 0.9221\n",
            "Epoch 177/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.1124 - acc: 0.9925 - val_loss: 0.3547 - val_acc: 0.9272\n",
            "Epoch 178/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.1107 - acc: 0.9930 - val_loss: 0.4594 - val_acc: 0.9063\n",
            "Epoch 179/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0956 - acc: 0.9972 - val_loss: 0.3262 - val_acc: 0.9348\n",
            "Epoch 180/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0922 - acc: 0.9977 - val_loss: 0.3398 - val_acc: 0.9310\n",
            "Epoch 181/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0909 - acc: 0.9977 - val_loss: 0.3528 - val_acc: 0.9289\n",
            "Epoch 182/250\n",
            "586/586 [==============================] - 328s 559ms/step - loss: 0.0901 - acc: 0.9974 - val_loss: 0.3332 - val_acc: 0.9327\n",
            "Epoch 183/250\n",
            "586/586 [==============================] - 328s 559ms/step - loss: 0.0839 - acc: 0.9992 - val_loss: 0.3482 - val_acc: 0.9308\n",
            "Epoch 184/250\n",
            "586/586 [==============================] - 328s 559ms/step - loss: 0.0816 - acc: 0.9993 - val_loss: 0.3214 - val_acc: 0.9356\n",
            "Epoch 185/250\n",
            "586/586 [==============================] - 328s 559ms/step - loss: 0.0798 - acc: 0.9993 - val_loss: 0.3222 - val_acc: 0.9390\n",
            "Epoch 186/250\n",
            "586/586 [==============================] - 327s 559ms/step - loss: 0.0772 - acc: 0.9995 - val_loss: 0.3684 - val_acc: 0.9322\n",
            "Epoch 187/250\n",
            "586/586 [==============================] - 327s 559ms/step - loss: 0.0759 - acc: 0.9997 - val_loss: 0.3177 - val_acc: 0.9401\n",
            "Epoch 188/250\n",
            "586/586 [==============================] - 327s 559ms/step - loss: 0.0748 - acc: 0.9999 - val_loss: 0.3146 - val_acc: 0.9407\n",
            "Epoch 189/250\n",
            "586/586 [==============================] - 328s 559ms/step - loss: 0.0744 - acc: 1.0000 - val_loss: 0.3144 - val_acc: 0.9407\n",
            "Epoch 190/250\n",
            "586/586 [==============================] - 328s 559ms/step - loss: 0.0738 - acc: 1.0000 - val_loss: 0.3143 - val_acc: 0.9409\n",
            "Epoch 191/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0738 - acc: 1.0000 - val_loss: 0.3143 - val_acc: 0.9413\n",
            "Epoch 192/250\n",
            "586/586 [==============================] - 327s 559ms/step - loss: 0.0735 - acc: 1.0000 - val_loss: 0.3141 - val_acc: 0.9416\n",
            "Epoch 193/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0733 - acc: 1.0000 - val_loss: 0.3135 - val_acc: 0.9412\n",
            "Epoch 194/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0729 - acc: 1.0000 - val_loss: 0.3143 - val_acc: 0.9411\n",
            "Epoch 195/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0730 - acc: 1.0000 - val_loss: 0.3144 - val_acc: 0.9415\n",
            "Epoch 196/250\n",
            "586/586 [==============================] - 327s 557ms/step - loss: 0.0727 - acc: 1.0000 - val_loss: 0.3140 - val_acc: 0.9406\n",
            "Epoch 197/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0725 - acc: 1.0000 - val_loss: 0.3119 - val_acc: 0.9410\n",
            "Epoch 198/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0722 - acc: 1.0000 - val_loss: 0.3144 - val_acc: 0.9404\n",
            "Epoch 199/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0722 - acc: 1.0000 - val_loss: 0.3141 - val_acc: 0.9413\n",
            "Epoch 200/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0720 - acc: 1.0000 - val_loss: 0.3139 - val_acc: 0.9408\n",
            "Epoch 201/250\n",
            "586/586 [==============================] - 328s 559ms/step - loss: 0.0718 - acc: 1.0000 - val_loss: 0.3143 - val_acc: 0.9410\n",
            "Epoch 202/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0715 - acc: 1.0000 - val_loss: 0.3135 - val_acc: 0.9407\n",
            "Epoch 203/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0715 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9407\n",
            "Epoch 204/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.0713 - acc: 1.0000 - val_loss: 0.3139 - val_acc: 0.9410\n",
            "Epoch 205/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.0711 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9410\n",
            "Epoch 206/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0709 - acc: 1.0000 - val_loss: 0.3148 - val_acc: 0.9409\n",
            "Epoch 207/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.0708 - acc: 1.0000 - val_loss: 0.3138 - val_acc: 0.9407\n",
            "Epoch 208/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0706 - acc: 1.0000 - val_loss: 0.3141 - val_acc: 0.9404\n",
            "Epoch 209/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0705 - acc: 1.0000 - val_loss: 0.3143 - val_acc: 0.9408\n",
            "Epoch 210/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0702 - acc: 1.0000 - val_loss: 0.3140 - val_acc: 0.9407\n",
            "Epoch 211/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0702 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.9410\n",
            "Epoch 212/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.0700 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9400\n",
            "Epoch 213/250\n",
            "586/586 [==============================] - 328s 561ms/step - loss: 0.0698 - acc: 1.0000 - val_loss: 0.3129 - val_acc: 0.9408\n",
            "Epoch 214/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0696 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.9408\n",
            "Epoch 215/250\n",
            "586/586 [==============================] - 328s 561ms/step - loss: 0.0695 - acc: 1.0000 - val_loss: 0.3142 - val_acc: 0.9409\n",
            "Epoch 216/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.0693 - acc: 1.0000 - val_loss: 0.3151 - val_acc: 0.9412\n",
            "Epoch 217/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.0692 - acc: 1.0000 - val_loss: 0.3138 - val_acc: 0.9408\n",
            "Epoch 218/250\n",
            "586/586 [==============================] - 330s 562ms/step - loss: 0.0689 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.9412\n",
            "Epoch 219/250\n",
            "586/586 [==============================] - 331s 565ms/step - loss: 0.0689 - acc: 1.0000 - val_loss: 0.3138 - val_acc: 0.9410\n",
            "Epoch 220/250\n",
            "586/586 [==============================] - 331s 565ms/step - loss: 0.0687 - acc: 1.0000 - val_loss: 0.3136 - val_acc: 0.9409\n",
            "Epoch 221/250\n",
            "586/586 [==============================] - 330s 563ms/step - loss: 0.0686 - acc: 1.0000 - val_loss: 0.3147 - val_acc: 0.9406\n",
            "Epoch 222/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0683 - acc: 1.0000 - val_loss: 0.3143 - val_acc: 0.9409\n",
            "Epoch 223/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0683 - acc: 1.0000 - val_loss: 0.3145 - val_acc: 0.9406\n",
            "Epoch 224/250\n",
            "586/586 [==============================] - 328s 561ms/step - loss: 0.0681 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.9407\n",
            "Epoch 225/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0679 - acc: 1.0000 - val_loss: 0.3142 - val_acc: 0.9409\n",
            "Epoch 226/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0677 - acc: 1.0000 - val_loss: 0.3129 - val_acc: 0.9409\n",
            "Epoch 227/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0676 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.9410\n",
            "Epoch 228/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0675 - acc: 1.0000 - val_loss: 0.3134 - val_acc: 0.9401\n",
            "Epoch 229/250\n",
            "586/586 [==============================] - 328s 560ms/step - loss: 0.0673 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9405\n",
            "Epoch 230/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0671 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.9405\n",
            "Epoch 231/250\n",
            "586/586 [==============================] - 329s 562ms/step - loss: 0.0670 - acc: 1.0000 - val_loss: 0.3126 - val_acc: 0.9403\n",
            "Epoch 232/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0668 - acc: 1.0000 - val_loss: 0.3145 - val_acc: 0.9402\n",
            "Epoch 233/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0667 - acc: 1.0000 - val_loss: 0.3135 - val_acc: 0.9402\n",
            "Epoch 234/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0665 - acc: 1.0000 - val_loss: 0.3140 - val_acc: 0.9404\n",
            "Epoch 235/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0664 - acc: 1.0000 - val_loss: 0.3128 - val_acc: 0.9407\n",
            "Epoch 236/250\n",
            "586/586 [==============================] - 329s 561ms/step - loss: 0.0662 - acc: 1.0000 - val_loss: 0.3142 - val_acc: 0.9406\n",
            "Epoch 237/250\n",
            "586/586 [==============================] - 328s 561ms/step - loss: 0.0661 - acc: 1.0000 - val_loss: 0.3135 - val_acc: 0.9403\n",
            "Epoch 238/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0659 - acc: 1.0000 - val_loss: 0.3141 - val_acc: 0.9408\n",
            "Epoch 239/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0658 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9408\n",
            "Epoch 240/250\n",
            "586/586 [==============================] - 327s 557ms/step - loss: 0.0656 - acc: 1.0000 - val_loss: 0.3124 - val_acc: 0.9401\n",
            "Epoch 241/250\n",
            "586/586 [==============================] - 327s 557ms/step - loss: 0.0655 - acc: 1.0000 - val_loss: 0.3126 - val_acc: 0.9403\n",
            "Epoch 242/250\n",
            "586/586 [==============================] - 327s 557ms/step - loss: 0.0653 - acc: 1.0000 - val_loss: 0.3142 - val_acc: 0.9408\n",
            "Epoch 243/250\n",
            "586/586 [==============================] - 327s 558ms/step - loss: 0.0652 - acc: 1.0000 - val_loss: 0.3126 - val_acc: 0.9405\n",
            "Epoch 244/250\n",
            "586/586 [==============================] - 326s 556ms/step - loss: 0.0651 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.9401\n",
            "Epoch 245/250\n",
            "586/586 [==============================] - 326s 556ms/step - loss: 0.0649 - acc: 1.0000 - val_loss: 0.3131 - val_acc: 0.9409\n",
            "Epoch 246/250\n",
            "586/586 [==============================] - 326s 557ms/step - loss: 0.0647 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.9404\n",
            "Epoch 247/250\n",
            "586/586 [==============================] - 326s 557ms/step - loss: 0.0646 - acc: 1.0000 - val_loss: 0.3124 - val_acc: 0.9406\n",
            "Epoch 248/250\n",
            "586/586 [==============================] - 326s 557ms/step - loss: 0.0645 - acc: 1.0000 - val_loss: 0.3128 - val_acc: 0.9404\n",
            "Epoch 249/250\n",
            "586/586 [==============================] - 326s 557ms/step - loss: 0.0644 - acc: 1.0000 - val_loss: 0.3132 - val_acc: 0.9410\n",
            "Epoch 250/250\n",
            "586/586 [==============================] - 326s 556ms/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.3130 - val_acc: 0.9406\n",
            "10000/10000 [==============================] - 19s 2ms/step\n",
            "Test loss: 0.31297942900657655\n",
            "Test accuracy: 0.9406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og56VCRh5j8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}